import torch
import argparse
import onnx
import yaml
import os
from pathlib import Path
from typing import Tuple, Dict, Optional, Union, List, Any

from models.model import StyleTransfer
from onnx import shape_inference
from hyperparam.hyperparam import Hyperparameter

# å¿½ç•¥ONNXè­¦å‘Š
import warnings
warnings.filterwarnings(
    "ignore",
    message="Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.",
    category=UserWarning,
    module="torch.onnx"
)


def load_config(config_path: str) -> Hyperparameter:
    """
    ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®å¹¶è½¬æ¢ä¸ºHyperparameterå¯¹è±¡
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        è§£æåçš„Hyperparameterå¯¹è±¡
    """
    with open(config_path, encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    # è½¬æ¢ä¸ºHyperparameterå¯¹è±¡
    return Hyperparameter(**config)


def export_onnx(
    model: torch.nn.Module,
    dummy_input: Tuple[torch.Tensor, torch.Tensor],
    output_path: str,
    dynamic_axes: Optional[Dict] = None,
    opset: int = 16
) -> bool:
    """
    å°†æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼
    
    Args:
        model: PyTorchæ¨¡å‹
        dummy_input: æ¨¡æ‹Ÿè¾“å…¥(å†…å®¹å›¾åƒ,é£æ ¼å›¾åƒ)
        output_path: è¾“å‡ºONNXæ–‡ä»¶è·¯å¾„
        dynamic_axes: åŠ¨æ€è½´é…ç½®(å¯é€‰)
        opset: ONNXæ“ä½œé›†ç‰ˆæœ¬
        
    Returns:
        å¯¼å‡ºæ˜¯å¦æˆåŠŸ
    """
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        # æ‰§è¡ŒONNXå¯¼å‡º
        torch.onnx.export(
            model=model,
            args=dummy_input,
            f=output_path,
            input_names=["content", "style"],
            output_names=["output"],
            dynamic_axes=dynamic_axes,
            opset_version=opset,
            do_constant_folding=True,
            training=torch.onnx.TrainingMode.EVAL,
            verbose=False
        )
        
        # å½¢çŠ¶æ¨ç†
        model_onnx = onnx.load(output_path)
        inferred_model = shape_inference.infer_shapes(model_onnx)
        onnx.save(inferred_model, output_path)
        
        print(f"âœ… ONNXå¯¼å‡ºæˆåŠŸ: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}")
        return False


def get_groups_config(hyper_param: Hyperparameter) -> List[int]:
    """
    æ ¹æ®é…ç½®è·å–åˆ†ç»„å·ç§¯å‚æ•°
    
    Args:
        hyper_param: è¶…å‚æ•°é…ç½®
        
    Returns:
        åˆ†ç»„å·ç§¯å‚æ•°åˆ—è¡¨
    """
    # è®¡ç®—åˆ†ç»„å‚æ•°(å¦‚æœæœªæ˜ç¡®è®¾ç½®)
    if hyper_param.groups_list:
        return hyper_param.groups_list
    elif hyper_param.groups:
        # å¦‚æœgroupsæ˜¯å•ä¸ªæ•´æ•°,é‡å¤4æ¬¡
        if isinstance(hyper_param.groups, int):
            return [hyper_param.groups] * 4
        return hyper_param.groups
    else:
        # æ ¹æ®æ¯”ä¾‹è®¡ç®—
        base_channels = [512, 256, 128, 64]
        groups_list = [
            max(1, int(c * r)) 
            for c, r in zip(base_channels, hyper_param.groups_ratios)
        ]
        print(f"è‡ªåŠ¨è®¡ç®—çš„åˆ†ç»„å‚æ•°: {groups_list}")
        return groups_list


def initialize_model(hyper_param: Hyperparameter, device: str) -> torch.nn.Module:
    """
    åˆå§‹åŒ–å…·æœ‰æ­£ç¡®å‚æ•°çš„æ¨¡å‹
    
    Args:
        hyper_param: è¶…å‚æ•°é…ç½®
        device: è®¾å¤‡('cuda'æˆ–'cpu')
        
    Returns:
        åˆå§‹åŒ–çš„æ¨¡å‹
    """
    # è·å–åˆ†ç»„é…ç½®
    groups_list = get_groups_config(hyper_param)
    
    # åˆ›å»ºå¯¼å‡ºé…ç½®
    export_config = {
        'export_mode': True,
        'fixed_batch_size': hyper_param.fixed_batch_size,
        'use_fixed_size': hyper_param.use_fixed_size
    }

    # åˆå§‹åŒ–æ¨¡å‹
    model = StyleTransfer(
        image_shape=hyper_param.image_shape,
        style_dim=hyper_param.style_dim,
        style_kernel=hyper_param.style_kernel,
        groups=groups_list,
        export_config=export_config
    ).to(device)
    
    return model


def load_checkpoint(model: torch.nn.Module, checkpoint_path: str, device: str):
    """
    ä»æ£€æŸ¥ç‚¹åŠ è½½æƒé‡(åŒ…å«å…¼å®¹æ€§å¤„ç†)
    
    Args:
        model: æ¨¡å‹
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¾å¤‡('cuda'æˆ–'cpu')
        
    Returns:
        åŠ è½½æƒé‡åçš„æ¨¡å‹
    """
    try:
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # æå–çŠ¶æ€å­—å…¸(å…¼å®¹ä¸åŒæ ¼å¼)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        
        # åŠ è½½çŠ¶æ€å­—å…¸(ä½¿ç”¨strict=Falseå…è®¸ç¼ºå¤±å‚æ•°)
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        if missing:
            print(f"â“ ç¼ºå¤±å‚æ•°: {len(missing)} ä¸ª")
        if unexpected:
            print(f"âš ï¸ æ„å¤–å‚æ•°: {len(unexpected)} ä¸ª")
                
        print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {checkpoint_path}")
        return model
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {str(e)}")
        raise


def main(
    checkpoint_path: str,
    config_path: str,
    output_path: str,
    opset: int = 16
):
    """
    ä¸»å¯¼å‡ºå‡½æ•°
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºONNXæ–‡ä»¶è·¯å¾„
        opset: ONNXæ“ä½œé›†ç‰ˆæœ¬
    """
    # åŠ è½½é…ç½®
    print(f"ğŸ”§ æ­£åœ¨åŠ è½½é…ç½®: {config_path}")
    hyper_param = load_config(config_path)
    
    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device.upper()}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    model = initialize_model(hyper_param, device)
    
    # åŠ è½½æƒé‡
    print(f"ğŸ”§ æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    model = load_checkpoint(model, checkpoint_path, device)
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # ç”Ÿæˆæ¨¡æ‹Ÿè¾“å…¥
    print(f"ğŸ”§ æ­£åœ¨å‡†å¤‡æ¨¡æ‹Ÿè¾“å…¥...")
    batch_size = hyper_param.fixed_batch_size if hyper_param.fixed_batch_size else 1
    dummy_content = torch.randn(batch_size, 3, *hyper_param.image_shape, device=device)
    dummy_style = torch.randn(batch_size, 3, *hyper_param.image_shape, device=device)
    
    # åŠ¨æ€è½´é…ç½®
    dynamic_axes = None
    if not hyper_param.use_fixed_size:
        dynamic_axes = {
            'content': {2: 'height', 3: 'width'},
            'style': {2: 'height', 3: 'width'},
            'output': {2: 'height', 3: 'width'}
        }
        
        # å¦‚æœfixed_batch_sizeæœªè®¾ç½®ï¼Œåˆ™æ‰¹æ¬¡å¤§å°ä¹Ÿæ˜¯åŠ¨æ€çš„
        if not hyper_param.fixed_batch_size:
            dynamic_axes['content'][0] = 'batch_size'
            dynamic_axes['style'][0] = 'batch_size'
            dynamic_axes['output'][0] = 'batch_size'
    
    # æ‰§è¡Œå¯¼å‡º
    print(f"ğŸ”§ æ­£åœ¨å¯¼å‡ºæ¨¡å‹...")
    success = export_onnx(
        model=model,
        dummy_input=(dummy_content, dummy_style),
        output_path=output_path,
        dynamic_axes=dynamic_axes,
        opset=opset
    )
    
    if success:
        # è·å–å¯¼å‡ºæ–‡ä»¶å¤§å°
        file_size = Path(output_path).stat().st_size / (1024 * 1024)  # MB
        print(f"ğŸ“Š å¯¼å‡ºæ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        # æ‰“å°å¯¼å‡ºæ¨¡å¼æ€»ç»“
        mode_str = "é™æ€æ¨¡å¼" if hyper_param.use_fixed_size else "åŠ¨æ€æ¨¡å¼"
        batch_str = f"å›ºå®šæ‰¹æ¬¡å¤§å°: {batch_size}" if hyper_param.fixed_batch_size else "åŠ¨æ€æ‰¹æ¬¡å¤§å°"
        shape_str = f"å›ºå®šç©ºé—´å°ºå¯¸: {hyper_param.image_shape}" if hyper_param.use_fixed_size else "åŠ¨æ€ç©ºé—´å°ºå¯¸"
        
        print(f"âœ… å¯¼å‡ºå®Œæˆ: {output_path}")
        print(f"   - æ¨¡å¼: {mode_str}")
        print(f"   - {batch_str}")
        print(f"   - {shape_str}")
        print(f"   - ONNXæ“ä½œé›†: {opset}")
    else:
        print(f"âŒ å¯¼å‡ºå¤±è´¥")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AdaConv ONNXå¯¼å‡ºå·¥å…·")
    parser.add_argument("--checkpoint", required=True, help="PyTorchæ£€æŸ¥ç‚¹è·¯å¾„(.pt)")
    parser.add_argument("--config", default="configs/lambda100.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", required=True, help="è¾“å‡ºONNXæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--opset", type=int, default=16, help="ONNXæ“ä½œé›†ç‰ˆæœ¬(é»˜è®¤: 16)")
    
    args = parser.parse_args()
    
    try:
        main(
            checkpoint_path=args.checkpoint,
            config_path=args.config,
            output_path=args.output,
            opset=args.opset
        )
    except Exception as e:
        import traceback
        print(f"âŒ å¯¼å‡ºè¿‡ç¨‹ç»ˆæ­¢ï¼Œå‡ºç°å¼‚å¸¸:")
        traceback.print_exc()
        exit(1)