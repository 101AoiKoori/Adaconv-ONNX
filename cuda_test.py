import torch
import onnxruntime as ort
from datetime import datetime
import subprocess
import platform
import sys
import os

def get_system_info():
    """è·å–ç³»ç»ŸåŸºæœ¬ä¿¡æ¯"""
    print("="*50)
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ“ä½œç³»ç»Ÿ: {platform.platform()}")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ONNXRuntimeç‰ˆæœ¬: {ort.__version__}")
    print("="*50 + "\n")

def check_nvidia_driver():
    """æ£€æŸ¥NVIDIAé©±åŠ¨"""
    print("[1/6] æ£€æŸ¥NVIDIAæ˜¾å¡é©±åŠ¨...")
    try:
        result = subprocess.check_output(["nvidia-smi"], shell=True, stderr=subprocess.STDOUT)
        print("âœ… æ£€æµ‹åˆ°NVIDIAé©±åŠ¨:")
        print(result.decode().strip())
        return True
    except Exception as e:
        print("âŒ æœªæ£€æµ‹åˆ°NVIDIAé©±åŠ¨æˆ–nvidia-smiä¸å¯ç”¨")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        return False

def check_cuda_toolkit():
    """æ£€æŸ¥CUDAå·¥å…·åŒ…"""
    print("\n[2/6] æ£€æŸ¥CUDAå·¥å…·åŒ…...")
    try:
        cuda_version = subprocess.check_output(["nvcc", "--version"]).decode()
        print("âœ… æ£€æµ‹åˆ°CUDAå·¥å…·åŒ…:")
        print(cuda_version.split("release")[-1].strip())
        return True
    except Exception as e:
        print("âŒ æœªæ£€æµ‹åˆ°CUDAå·¥å…·åŒ…æˆ–nvccä¸å¯ç”¨")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        return False

def check_pytorch_cuda():
    """æ£€æŸ¥PyTorch CUDAæ”¯æŒ"""
    print("\n[3/6] æ£€æŸ¥PyTorch CUDAæ”¯æŒ...")
    
    # åŸºç¡€æ£€æŸ¥
    cuda_available = torch.cuda.is_available()
    print(f"PyTorch CUDAå¯ç”¨: {'âœ…' if cuda_available else 'âŒ'}")
    
    if cuda_available:
        # è®¾å¤‡ä¿¡æ¯
        device_count = torch.cuda.device_count()
        print(f"æ£€æµ‹åˆ° {device_count} ä¸ªCUDAè®¾å¤‡")
        
        for i in range(device_count):
            print(f"\nè®¾å¤‡ {i}:")
            print(f"  åç§°: {torch.cuda.get_device_name(i)}")
            print(f"  Compute Capability: {torch.cuda.get_device_capability(i)}")
            print(f"  æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(i).total_memory/1024**3:.2f} GB")
        
        # ç‰ˆæœ¬ä¿¡æ¯
        print("\nCUDAç‰ˆæœ¬:")
        print(f"  PyTorchå†…ç½®CUDAç‰ˆæœ¬: {torch.version.cuda}")
        if hasattr(torch.cuda, 'get_driver_version'):
            print(f"  å½“å‰é©±åŠ¨ç‰ˆæœ¬: {torch.cuda.get_driver_version()}")
        else:
            print("  å½“å‰é©±åŠ¨ç‰ˆæœ¬: (é€šè¿‡PyTorchæ— æ³•è·å–ï¼Œè¯·ç›´æ¥è¿è¡Œ`nvidia-smi`æŸ¥çœ‹)")
                
        # æ‰§è¡Œè®¡ç®—æµ‹è¯•
        print("\næ‰§è¡Œè®¡ç®—æµ‹è¯•...")
        try:
            x = torch.randn(1000, 1000).cuda()
            y = torch.randn(1000, 1000).cuda()
            z = (x @ y).mean()
            print(f"âœ… è®¡ç®—æµ‹è¯•é€šè¿‡: ç»“æœå€¼={z.item():.4f}")
            return True
        except Exception as e:
            print(f"âŒ è®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
    return False

def check_onnxruntime_cuda():
    """æ£€æŸ¥ONNX Runtime CUDAæ”¯æŒ"""
    print("\n[4/6] æ£€æŸ¥ONNX Runtime CUDAæ”¯æŒ...")
    
    # è·å–å¯ç”¨provider
    providers = ort.get_available_providers()
    print("å¯ç”¨è®¡ç®—åç«¯:", providers)
    
    if 'CUDAExecutionProvider' not in providers:
        print("âŒ æœªæ£€æµ‹åˆ°CUDAExecutionProvider")
        return False
    
    # åˆ›å»ºCUDA session
    print("\nåˆ›å»ºCUDAä¼šè¯...")
    try:
        options = ort.SessionOptions()
        session = ort.InferenceSession(
            "model.onnx" if os.path.exists("model.onnx") else "example.onnx",
            providers=['CUDAExecutionProvider'],
            sess_options=options
        )
        print("âœ… CUDAä¼šè¯åˆ›å»ºæˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ CUDAä¼šè¯åˆ›å»ºå¤±è´¥: {str(e)}")
        return False

def check_cudnn():
    """æ£€æŸ¥cuDNNå®‰è£…"""
    print("\n[5/6] æ£€æŸ¥cuDNNå®‰è£…...")
    try:
        from torch.backends import cudnn
        print(f"PyTorchä½¿ç”¨çš„cuDNNç‰ˆæœ¬: {cudnn.version()}")
        
        # æ‰§è¡ŒcuDNNåŠ é€Ÿçš„å·ç§¯æ“ä½œ
        input = torch.randn(1,3,224,224).cuda()
        conv = torch.nn.Conv2d(3, 64, kernel_size=3).cuda()
        output = conv(input)
        print("âœ… cuDNNè®¡ç®—æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ cuDNNæ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def check_environment_variables():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡"""
    print("\n[6/6] æ£€æŸ¥ç¯å¢ƒå˜é‡...")
    required_paths = [
        "CUDA_PATH",
        "PATH",
        "LD_LIBRARY_PATH" if platform.system() != "Windows" else ""
    ]
    
    for var in required_paths:
        if not var: continue
        value = os.environ.get(var, "")
        print(f"{var}:")
        print("  " + "\n  ".join(value.split(os.pathsep)) if value else print("  (æœªè®¾ç½®)"))

def main():
    get_system_info()
    results = {
        "nvidia_driver": check_nvidia_driver(),
        "cuda_toolkit": check_cuda_toolkit(),
        "pytorch_cuda": check_pytorch_cuda(),
        "onnxruntime_cuda": check_onnxruntime_cuda(),
        "cudnn": check_cudnn()
    }
    
    check_environment_variables()
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "="*50)
    print("CUDAç¯å¢ƒéªŒè¯æ€»ç»“:")
    for name, status in results.items():
        print(f"{name.replace('_', ' ').title():<20} : {'âœ…' if status else 'âŒ'}")
    
    if all(results.values()):
        print("\nğŸ‰ æ‰€æœ‰æ£€æŸ¥é¡¹é€šè¿‡ï¼CUDAç¯å¢ƒé…ç½®æ­£ç¡®")
    else:
        print("\nâš ï¸ å­˜åœ¨æœªé€šè¿‡çš„æ£€æŸ¥é¡¹ï¼Œè¯·æ ¹æ®ä»¥ä¸Šè¾“å‡ºæ’æŸ¥é—®é¢˜")

if __name__ == "__main__":
    main()